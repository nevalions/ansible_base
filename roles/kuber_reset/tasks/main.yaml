---
- name: Check if Kubernetes configuration exists
  ansible.builtin.stat:
    path: "{{ item }}"
  register: k8s_config_exists
  loop:
    - /etc/kubernetes/admin.conf
    - /etc/kubernetes/kubelet.conf
  failed_when: false

- name: Display Kubernetes configuration status
  ansible.builtin.debug:
    msg:
      - "=== Kubernetes Reset Start ==="
      - "Kubernetes admin.conf exists: {{ k8s_config_exists.results[0].stat.exists }}"
      - "Kubernetes kubelet.conf exists: {{ k8s_config_exists.results[1].stat.exists }}"

- name: Check if Kubernetes packages are installed
  ansible.builtin.apt:
    name:
      - kubelet
      - kubeadm
      - kubectl
    state: absent
  check_mode: yes
  register: k8s_packages_installed
  changed_when: false
  failed_when: false

- name: Check if kubeadm reset is available before package removal
  ansible.builtin.command: which kubeadm
  register: kubeadm_check_before_removal
  changed_when: false
  failed_when: false

- name: Run kubeadm reset before package removal
  ansible.builtin.command: kubeadm reset --force
  when: kubeadm_check_before_removal.rc == 0
  failed_when: false
  register: kubeadm_reset_before_removal
  changed_when: kubeadm_reset_before_removal.rc == 0

- name: Unhold Kubernetes packages
  ansible.builtin.dpkg_selections:
    name: "{{ item }}"
    selection: install
  loop:
    - kubelet
    - kubeadm
    - kubectl
  when: remove_kubernetes_packages | default(true)
  failed_when: false

- name: Remove Kubernetes packages
  ansible.builtin.apt:
    name:
      - kubelet
      - kubeadm
      - kubectl
    state: absent
    purge: yes
    autoremove: yes
  when: remove_kubernetes_packages | default(true)
  register: pkg_removal
  failed_when: false

- name: Stop kubelet service
  ansible.builtin.systemd:
    name: kubelet
    state: stopped
    enabled: false
  failed_when: false

- name: Stop NFS server related services when present
  ansible.builtin.systemd:
    name: "{{ item }}"
    state: stopped
    enabled: false
  loop:
    - nfs-kernel-server
    - nfs-server
    - rpcbind
    - rpcbind.socket
  failed_when: false

- name: Stop containerd service temporarily for cleanup
  ansible.builtin.systemd:
    name: containerd
    state: stopped
  failed_when: false

- name: Remove Kubernetes static pod manifests
  ansible.builtin.file:
    path: /etc/kubernetes/manifests
    state: absent
  failed_when: false

- name: Stop remaining control plane processes
  ansible.builtin.shell: |
    pkill -f kube-apiserver || true
    pkill -f kube-controller-manager || true
    pkill -f kube-scheduler || true
    pkill -f etcd || true
  failed_when: false
  changed_when: false

- name: Stop CNI-related services
  ansible.builtin.shell: |
    systemctl stop cni* 2>/dev/null || true
    pkill -9 -f cni 2>/dev/null || true
  failed_when: false
  changed_when: false

- name: Capture listeners on MetalLB memberlist port before cleanup
  ansible.builtin.shell: ss -H -ltnup '( sport = :7946 )' || true
  register: metallb_memberlist_listeners_before
  failed_when: false
  changed_when: false

- name: Capture listeners on CSI sidecar ports before cleanup
  ansible.builtin.shell: ss -H -ltnup '( sport = :19809 or sport = :29653 )' || true
  register: csi_sidecar_listeners_before
  failed_when: false
  changed_when: false

- name: Kill stale listeners on CSI sidecar ports
  ansible.builtin.shell: |
    set -e -o pipefail
    pids="$(ss -H -ltnup '( sport = :19809 or sport = :29653 )' | awk -F 'pid=' '{for(i=2;i<=NF;i++){split($i,a,","); print a[1]}}' | sort -u)"
    if [ -n "${pids}" ]; then
      kill -9 ${pids} || true
      printf '%s\n' "${pids}"
    fi
  register: csi_sidecar_killed_pids
  failed_when: false
  changed_when: csi_sidecar_killed_pids.stdout | trim | length > 0

- name: Kill stale listeners on MetalLB memberlist port
  ansible.builtin.shell: |
    set -e -o pipefail
    pids="$(ss -H -ltnup '( sport = :7946 )' | awk -F 'pid=' '{for(i=2;i<=NF;i++){split($i,a,","); print a[1]}}' | sort -u)"
    if [ -n "${pids}" ]; then
      kill -9 ${pids} || true
      printf '%s\n' "${pids}"
    fi
  register: metallb_memberlist_killed_pids
  failed_when: false
  changed_when: metallb_memberlist_killed_pids.stdout | trim | length > 0

- name: Check if cni0 interface exists
  ansible.builtin.command: ip link show cni0
  register: cni0_interface
  changed_when: false
  failed_when: false

- name: Remove cni0 interface explicitly
  ansible.builtin.command: ip link delete cni0
  when: cni0_interface.rc == 0
  failed_when: false
  changed_when: true

- name: Check if flannel.1 interface exists
  ansible.builtin.command: ip link show flannel.1
  register: flannel_interface
  changed_when: false
  failed_when: false

- name: Remove flannel.1 interface explicitly
  ansible.builtin.command: ip link delete flannel.1
  when: flannel_interface.rc == 0
  failed_when: false
  changed_when: true

- name: Force remove CNI directory contents
  ansible.builtin.shell: |
    rm -rf /etc/cni/net.d/* 2>/dev/null || true
    rm -rf /opt/cni/bin/* 2>/dev/null || true
    rm -rf /var/lib/cni/* 2>/dev/null || true
  failed_when: false
  changed_when: false

- name: Remove CNI plugins and configurations
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/cni/net.d
    - /opt/cni/bin
    - /var/lib/cni

- name: Remove CNI network interfaces (Calico and Flannel)
  ansible.builtin.shell: |
    set -e -o pipefail
    ip link show | grep -E "cali|tunl|flannel|cni0|vxlan" | awk '{print $2}' | cut -d':' -f1 | cut -d'@' -f1 | xargs -r -I {} ip link delete {} 2>/dev/null || true
  failed_when: false
  changed_when: false

- name: Ensure containerd is running for CRI cleanup
  ansible.builtin.systemd:
    name: containerd
    state: started
  failed_when: false

- name: Remove all Docker/Containerd containers
  ansible.builtin.shell: |
    set -e -o pipefail
    if command -v crictl &> /dev/null; then
      crictl ps -q | xargs -r crictl stop
      crictl ps -aq | xargs -r crictl rm
    fi
  failed_when: false
  changed_when: false
  register: container_cleanup

- name: Kill MetalLB memberlist listeners after container cleanup
  ansible.builtin.shell: |
    set -e -o pipefail
    pids="$(ss -H -ltnup '( sport = :7946 )' | awk -F 'pid=' '{for(i=2;i<=NF;i++){split($i,a,","); print a[1]}}' | sort -u)"
    if [ -n "${pids}" ]; then
      kill -9 ${pids} || true
      printf '%s\n' "${pids}"
    fi
  register: metallb_memberlist_killed_pids_after_cleanup
  failed_when: false
  changed_when: metallb_memberlist_killed_pids_after_cleanup.stdout | trim | length > 0

- name: Remove all container images (if configured)
  ansible.builtin.shell: |
    set -e -o pipefail
    if command -v crictl &> /dev/null; then
      crictl images -q | xargs -r crictl rmi
    fi
  when: remove_container_images | default(false)
  failed_when: false
  changed_when: false
  register: image_cleanup

- name: Clean Kubernetes data directories
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/kubernetes
    - /var/lib/kubelet
    - /var/lib/dockershim
    - /var/run/kubernetes
    - /var/lib/etcd

- name: Clean containerd data
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /var/lib/containerd
    - /etc/containerd

- name: Flush iptables rules
  ansible.builtin.shell: |
    iptables -F
    iptables -t nat -F
    iptables -t mangle -F
    iptables -X
    iptables -t nat -X
    iptables -t mangle -X
    iptables -P INPUT ACCEPT
    iptables -P FORWARD ACCEPT
    iptables -P OUTPUT ACCEPT
  failed_when: false
  changed_when: false

- name: Flush ip6tables rules
  ansible.builtin.shell: |
    ip6tables -F
    ip6tables -t nat -F
    ip6tables -t mangle -F
    ip6tables -X
    ip6tables -t nat -X
    ip6tables -t mangle -X
    ip6tables -P INPUT ACCEPT
    ip6tables -P FORWARD ACCEPT
    ip6tables -P OUTPUT ACCEPT
  failed_when: false
  changed_when: false

- name: Clean kube-proxy iptables rules
  ansible.builtin.shell: |
    set -e -o pipefail
    iptables-save | grep -v KUBE | grep -v CNI | iptables-restore
  failed_when: false
  changed_when: false

- name: Stop CNI network namespace systemd units
  ansible.builtin.shell: |
    set -e -o pipefail
    systemctl list-units --all --type=mount | grep 'run-netns-cni' | awk '{print $1}' | xargs -r systemctl stop
  failed_when: false
  changed_when: false
  register: cni_systemd_stop

- name: Unmount CNI network namespace mounts
  ansible.builtin.shell: |
    set -e -o pipefail
    mount | grep '/run/netns/cni' | awk '{print $3}' | xargs -r -n1 umount -l
  failed_when: false
  changed_when: false
  register: cni_netns_unmount

- name: Remove CNI network namespace directories after unmount
  ansible.builtin.shell: |
    set -e -o pipefail
    rm -rf /run/netns/cni-* 2>/dev/null || true
  failed_when: false
  changed_when: false
  register: cni_netns_dirs

- name: Remove CNI network namespaces
  ansible.builtin.shell: |
    set -e -o pipefail
    ip netns list 2>/dev/null | grep cni | xargs -r -I {} ip netns delete {}
  failed_when: false
  changed_when: false
  register: cni_netns_delete

- name: Check for orphaned CNI network namespace mounts
  ansible.builtin.shell: |
    systemctl list-units --all --type=mount | grep 'run-netns-cni' | wc -l
  failed_when: false
  changed_when: false
  register: orphaned_netns_count

- name: Warn about orphaned network namespaces
  ansible.builtin.debug:
    msg:
      - "=== WARNING: Orphaned CNI Network Namespaces Detected ==="
      - "Found {{ orphaned_netns_count.stdout | int }} orphaned network namespace mount(s)"
      - "These orphaned nsfs mounts are systemd mount units that persist despite cleanup"
      - "They DO NOT interfere with Kubernetes operation or reinitialization"
      - "To remove them completely, reboot the node:"
      - "  sudo reboot"
      - "Reference: kubeadm reset is 'best-effort' cleanup - Kubernetes docs recommend manual intervention or reboot for persistent network namespace mounts"
  when: orphaned_netns_count.stdout | int > 0

- name: Clean any remaining Calico data (persistent)
  ansible.builtin.file:
    path: /var/lib/calico
    state: absent
  failed_when: false

- name: Check if /run/calico is a mount
  ansible.builtin.command: findmnt -n /run/calico
  register: calico_run_mount
  changed_when: false
  failed_when: false

- name: Clean any remaining Calico data (runtime)
  ansible.builtin.file:
    path: /run/calico
    state: absent
  failed_when: false
  when: calico_run_mount.rc != 0

- name: Warn if /run/calico is a mount
  ansible.builtin.debug:
    msg: "/run/calico is a runtime mount; skipping removal. A reboot clears it."
  when: calico_run_mount.rc == 0

- name: Clean any remaining Flannel data
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /run/flannel
    - /var/lib/cni/flannel
  failed_when: false

- name: Reload systemd daemon
  ansible.builtin.systemd:
    daemon_reload: true

- name: Verify CNI directories are removed (before containerd starts)
  ansible.builtin.stat:
    path: "{{ item }}"
  register: cni_dirs_removed
  failed_when: false
  loop:
    - /opt/cni/bin
    - /var/lib/cni

- name: Assert CNI runtime directories are removed
  ansible.builtin.assert:
    that:
      - not item.stat.exists
    success_msg: "{{ item.item }} removed successfully"
    fail_msg: "{{ item.item }} still exists after reset"
  loop: "{{ cni_dirs_removed.results }}"

- name: Check if /etc/cni/net.d exists after cleanup
  ansible.builtin.stat:
    path: /etc/cni/net.d
  register: cni_netd_dir
  failed_when: false

- name: List /etc/cni/net.d entries when directory exists
  ansible.builtin.find:
    paths: /etc/cni/net.d
    file_type: any
    recurse: false
  register: cni_netd_entries
  when: cni_netd_dir.stat.exists
  failed_when: false

- name: Assert /etc/cni/net.d is absent or empty
  ansible.builtin.assert:
    that:
      - not cni_netd_dir.stat.exists or (cni_netd_entries.matched | default(0) | int) == 0
    success_msg: "/etc/cni/net.d is absent or empty (expected after reset)"
    fail_msg: >-
      /etc/cni/net.d contains CNI configs after reset:
      {{ cni_netd_entries.files | default([]) | map(attribute='path') | list | join(', ') }}.
      Remove stale CNI config files, then re-run kuber_worker_reset.yaml.

- name: Check listeners on MetalLB memberlist port after cleanup
  ansible.builtin.shell: ss -H -ltnup '( sport = :7946 )' || true
  register: metallb_memberlist_listeners_after
  failed_when: false
  changed_when: false

- name: Ensure kubelet stays stopped before CSI listener enforcement
  ansible.builtin.systemd:
    name: kubelet
    state: stopped
    enabled: false
  failed_when: false

- name: Stop containerd temporarily before CSI listener enforcement
  ansible.builtin.systemd:
    name: containerd
    state: stopped
  failed_when: false

- name: Force release CSI sidecar listener ports
  ansible.builtin.shell: |
    i=0
    while [ "$i" -lt 15 ]; do
      pids="$(ss -H -ltnup '( sport = :19809 or sport = :29653 )' | awk -F 'pid=' '{for(n=2;n<=NF;n++){split($n,a,","); print a[1]}}' | sort -u)"
      if [ -z "${pids}" ]; then
        break
      fi

      kill -9 ${pids} 2>/dev/null || true
      pkill -9 -f csi-node-driver 2>/dev/null || true
      pkill -9 -f livenessprobe 2>/dev/null || true
      pkill -9 -f node-driver-registrar 2>/dev/null || true
      sleep 1
      i=$((i + 1))
    done
  changed_when: csi_sidecar_listeners_before.stdout_lines | length > 0
  failed_when: false

- name: Refresh listeners on CSI sidecar ports after cleanup
  ansible.builtin.shell: ss -H -ltnup '( sport = :19809 or sport = :29653 )' || true
  register: csi_sidecar_listeners_after
  failed_when: false
  changed_when: false

- name: Assert CSI sidecar ports are free after reset
  ansible.builtin.assert:
    that:
      - csi_sidecar_listeners_after.stdout_lines | length == 0
    success_msg: "CSI sidecar ports 19809/29653 are free"
    fail_msg: >-
      CSI sidecar listener port(s) are still in use after reset.
      Listener(s): {{ csi_sidecar_listeners_after.stdout_lines | join('; ') }}.
      Stop conflicting process/service on this node, then run kuber_worker_reset.yaml again.
  when: kuber_reset_enforce_listener_cleanup | default(true)

- name: Ensure kubelet stays stopped before final MetalLB cleanup
  ansible.builtin.systemd:
    name: kubelet
    state: stopped
    enabled: false
  failed_when: false

- name: Force release MetalLB memberlist port listeners
  ansible.builtin.shell: |
    i=0
    while [ "$i" -lt 15 ]; do
      pids="$(ss -H -ltnup '( sport = :7946 )' | awk -F 'pid=' '{for(n=2;n<=NF;n++){split($n,a,","); print a[1]}}' | sort -u)"
      if [ -z "${pids}" ]; then
        break
      fi

      kill -9 ${pids} 2>/dev/null || true
      pkill -9 -x speaker 2>/dev/null || true
      pkill -9 -f metallb.*speaker 2>/dev/null || true
      sleep 1
      i=$((i + 1))
    done
  changed_when: metallb_memberlist_listeners_after.stdout_lines | length > 0
  failed_when: false

- name: Refresh listeners on MetalLB memberlist port after force cleanup
  ansible.builtin.shell: ss -H -ltnup '( sport = :7946 )' || true
  register: metallb_memberlist_listeners_after
  failed_when: false
  changed_when: false

- name: Assert MetalLB memberlist port is free after reset
  ansible.builtin.assert:
    that:
      - metallb_memberlist_listeners_after.stdout_lines | length == 0
    success_msg: "MetalLB memberlist port 7946 is free"
    fail_msg: >-
      Port 7946 is still in use after reset. Listener(s):
      {{ metallb_memberlist_listeners_after.stdout_lines | join('; ') }}.
      Stop conflicting service/process on this node, then run kuber_worker_reset.yaml again.

- name: Verify cni0 and flannel.1 interfaces are absent before containerd starts
  ansible.builtin.command: ip link show {{ item }}
  loop:
    - cni0
    - flannel.1
  register: cni_interfaces_absent
  changed_when: false
  failed_when: false

- name: Assert cni0 and flannel.1 interfaces are absent
  ansible.builtin.assert:
    that:
      - item.rc != 0
    success_msg: "{{ item.item }} interface is absent"
    fail_msg: "{{ item.item }} interface still exists after reset"
  loop: "{{ cni_interfaces_absent.results }}"

- name: Start containerd service
  ansible.builtin.systemd:
    name: containerd
    state: started
    enabled: true

- name: Verify Kubernetes directories are removed
  ansible.builtin.stat:
    path: "{{ item }}"
  register: k8s_dirs_removed
  failed_when: false
  loop:
    - /etc/kubernetes
    - /var/lib/kubelet
    - /var/lib/etcd

- name: Verify Calico and Flannel directories are removed
  ansible.builtin.stat:
    path: "{{ item }}"
  register: calico_dirs_removed
  failed_when: false
  loop:
    - /var/lib/calico
    - /run/flannel

- name: Check if /run/calico still exists
  ansible.builtin.stat:
    path: /run/calico
  register: calico_run_dir
  failed_when: false

- name: Verify CNI directories are removed
  ansible.builtin.stat:
    path: "{{ item }}"
  register: cni_dirs_removed
  failed_when: false
  loop:
    - /etc/cni/net.d
    - /opt/cni/bin
    - /var/lib/cni

- name: Assert Kubernetes directories are removed
  ansible.builtin.assert:
    that:
      - not item.stat.exists
    success_msg: "{{ item.item }} removed successfully"
    fail_msg: "{{ item.item }} still exists after reset"
  loop: "{{ k8s_dirs_removed.results }}"

- name: Assert Calico and Flannel directories are removed
  ansible.builtin.assert:
    that:
      - not item.stat.exists
    success_msg: "{{ item.item }} removed successfully"
    fail_msg: "{{ item.item }} still exists after reset"
  loop: "{{ calico_dirs_removed.results }}"

- name: Warn if /run/calico persists
  ansible.builtin.debug:
    msg: "/run/calico still exists (tmpfs/active runtime files). This is safe to ignore; reboot clears it."
  when: calico_run_dir.stat.exists | default(false)

- name: Display CNI recreation note
  ansible.builtin.debug:
    msg: "NOTE: Containerd may recreate empty /etc/cni/net.d directory on startup - this is expected behavior"

- name: Verify kubelet is stopped
  ansible.builtin.systemd:
    name: kubelet
  register: kubelet_status
  failed_when: false

- name: Verify containerd is running
  ansible.builtin.systemd:
    name: containerd
  register: containerd_running
  failed_when: false

- name: Capture TCP listeners on NFS ports
  ansible.builtin.shell: ss -H -ltnp '( sport = :111 or sport = :2049 )' || true
  register: nfs_tcp_listeners_after
  failed_when: false
  changed_when: false

- name: Capture UDP listeners on NFS ports
  ansible.builtin.shell: ss -H -lunp '( sport = :111 or sport = :2049 )' || true
  register: nfs_udp_listeners_after
  failed_when: false
  changed_when: false

- name: Assert NFS server ports are not listening on worker after reset
  ansible.builtin.assert:
    that:
      - nfs_tcp_listeners_after.stdout_lines | length == 0
      - nfs_udp_listeners_after.stdout_lines | length == 0
    success_msg: "No NFS/rpcbind listeners detected on ports 111/2049"
    fail_msg: >-
      NFS-related listener(s) still detected on ports 111/2049.
      TCP: {{ nfs_tcp_listeners_after.stdout_lines | join('; ') | default('none') }}
      UDP: {{ nfs_udp_listeners_after.stdout_lines | join('; ') | default('none') }}
  when: kuber_reset_enforce_listener_cleanup | default(true)

- name: Display reset completion message
  ansible.builtin.debug:
    msg:
      - "=== Kubernetes Reset Complete ==="
      - "Kubernetes directories: Removed"
      - "Calico/Flannel directories: Removed"
      - "CNI directories: Verified removed (before containerd start)"
      - "CNI network namespaces: {{ 'Cleaned' if orphaned_netns_count.stdout | int == 0 else 'Warning - see above' }}"
      - "Kubelet status: {{ 'Stopped (expected)' if kubelet_status.status.ActiveState != 'active' else 'WARNING: Still running!' }}"
      - "Containerd status: {{ 'Running' if containerd_running.status.ActiveState == 'active' else 'Not running' }}"
      - "Container images: {{ 'Removed' if remove_container_images | default(false) else 'Preserved' }}"
      - "Note: Containerd recreates /etc/cni/net.d on startup (expected)"
      - "Next step: Run kuber.yaml then kuber_plane_init.yaml or kuber_worker_join.yaml"
