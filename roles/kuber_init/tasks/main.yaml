---
- name: Check if Kubernetes is already initialized
  ansible.builtin.stat:
    path: /etc/kubernetes/admin.conf
  register: k8s_admin_conf

- name: Set initialized flag
  ansible.builtin.set_fact:
    kubeadm_cluster_initialized: "{{ k8s_admin_conf.stat.exists }}"

- name: Override kubeconfig user when root
  ansible.builtin.set_fact:
    kubeconfig_user: "{{ ansible_user }}"
  when:
    - kubeconfig_user == 'root'
    - ansible_user is defined
    - ansible_user != 'root'

- name: Fail if Kubernetes is already initialized
  ansible.builtin.fail:
    msg: "Kubernetes cluster is already initialized. Use kuber_plane_reset.yaml to reset first."
  when:
    - kubeadm_cluster_initialized
    - not (kubeadm_refresh_on_existing | default(true))

- name: Create containerd config directory
  ansible.builtin.file:
    path: /etc/containerd
    state: directory
    mode: "0755"

- name: Configure containerd cgroup driver
  ansible.builtin.template:
    src: "{{ playbook_dir }}/roles/kuber/templates/containerd-config.toml.j2"
    dest: /etc/containerd/config.toml
    mode: "0644"

- name: Restart containerd
  ansible.builtin.systemd:
    name: containerd
    state: restarted
    enabled: true

- name: Create kubelet systemd service file
  ansible.builtin.copy:
    content: |
      [Unit]
      Description=kubelet: The Kubernetes Node Agent
      Documentation=https://kubernetes.io/docs/
      After=containerd.service
      Requires=containerd.service

      [Service]
      ExecStart=/usr/local/bin/kubelet
      Restart=always
      StartLimitInterval=0
      RestartSec=10

      [Install]
      WantedBy=multi-user.target
    dest: /etc/systemd/system/kubelet.service
    mode: "0644"

- name: Create kubelet drop-in directory
  ansible.builtin.file:
    path: /etc/systemd/system/kubelet.service.d
    state: directory
    mode: "0755"

- name: Create kubeadm kubelet drop-in configuration
  ansible.builtin.copy:
    content: |
      [Service]
      Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
      Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
      # This is a file that "kubeadm init" and "kubeadm join" generate at runtime
      EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
      # This is a file that the user can use for overrides of the kubelet args
      EnvironmentFile=-/etc/default/kubelet
      ExecStart=
      ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
    dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
    mode: "0644"

- name: Reload systemd daemon
  ansible.builtin.systemd:
    daemon_reload: true

- name: Stop kubelet service (pre-init)
  ansible.builtin.systemd:
    name: kubelet
    state: stopped
  failed_when: false

- name: Ensure kubelet service is enabled and started (required for kubeadm init)
  ansible.builtin.systemd:
    name: kubelet
    enabled: true
    state: started
  when: not ansible_check_mode

- name: Copy kubeadm config
  ansible.builtin.template:
    src: kubeadm-config.yaml.j2
    dest: /tmp/kubeadm-config.yaml
    mode: "0644"

- name: Refresh control plane certificates and admin kubeconfig
  ansible.builtin.command: "{{ item }}"
  loop:
    - kubeadm init phase certs apiserver --config=/tmp/kubeadm-config.yaml
    - kubeadm init phase kubeconfig admin --config=/tmp/kubeadm-config.yaml
  register: kubeadm_refresh
  changed_when: true
  when:
    - kubeadm_cluster_initialized
    - kubeadm_refresh_on_existing | default(true)

- name: Check if Kubernetes API port is in use
  ansible.builtin.command: ss -ltnp
  register: k8s_api_port_check
  changed_when: false
  failed_when: false
  when:
    - not kubeadm_cluster_initialized

- name: Fail if Kubernetes API port is already in use
  ansible.builtin.fail:
    msg:
      - "Kubernetes API port {{ vault_k8s_api_port }} is already in use"
      - "Stop the service using the port or run kuber_plane_reset.yaml before init"
  when:
    - not kubeadm_cluster_initialized
    - k8s_api_port_check.stdout is search(':' ~ vault_k8s_api_port)

- name: Initialize Kubernetes control plane
  ansible.builtin.command: kubeadm init --config=/tmp/kubeadm-config.yaml
  register: kubeadm_init
  changed_when: true
  when:
    - not kubeadm_cluster_initialized

- name: Look up kubeconfig user home directory
  ansible.builtin.getent:
    database: passwd
    key: "{{ kubeconfig_user }}"
  register: kubeconfig_user_passwd

- name: Look up ansible user home directory
  ansible.builtin.getent:
    database: passwd
    key: "{{ ansible_user }}"
  register: ansible_user_passwd
  when:
    - ansible_user is defined
    - ansible_user != 'root'
    - ansible_user != kubeconfig_user

- name: Set kubeconfig user home and path
  ansible.builtin.set_fact:
    kubeconfig_user_home: >-
      {{
        (kubeconfig_user_passwd.ansible_facts.getent_passwd[kubeconfig_user][5]
        | default(kubeconfig_user_home))
      }}
    kubeconfig_path: "{{ kubeconfig_user_home }}/.kube/config"

- name: Set ansible user kubeconfig path
  ansible.builtin.set_fact:
    ansible_user_kubeconfig_path: >-
      {{
        (ansible_user_passwd.ansible_facts.getent_passwd[ansible_user][5]
        | default('/home/' ~ ansible_user))
      }}/.kube/config
  when:
    - ansible_user is defined
    - ansible_user != 'root'

- name: Build kubeconfig target list
  ansible.builtin.set_fact:
    kubeconfig_targets: >-
      {{
        (
          ([{'path': kubeconfig_path, 'owner': kubeconfig_user}]
          if (kubeconfig_user == 'root' or kubeconfig_path is not match('^/root/'))
          else [])
          +
          ([{'path': ansible_user_kubeconfig_path, 'owner': ansible_user}]
          if (ansible_user_kubeconfig_path is defined and ansible_user_kubeconfig_path | length > 0)
          else [])
        )
      }}

- name: Create .kube directory
  ansible.builtin.file:
    path: "{{ item.path | dirname }}"
    state: directory
    mode: "0700"
    owner: "{{ item.owner }}"
    group: "{{ item.owner }}"
  loop: "{{ kubeconfig_targets }}"
  when:
    - kubeconfig_targets is defined
    - kubeconfig_targets | length > 0

- name: Copy admin.conf to user's kubeconfig
  ansible.builtin.copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ item.path }}"
    remote_src: true
    mode: "0600"
    owner: "{{ item.owner }}"
    group: "{{ item.owner }}"
  loop: "{{ kubeconfig_targets }}"
  when:
    - kubeconfig_targets is defined
    - kubeconfig_targets | length > 0

- name: Ensure kubeconfig directory ownership and mode
  ansible.builtin.file:
    path: "{{ item.path | dirname }}"
    state: directory
    owner: "{{ item.owner }}"
    group: "{{ item.owner }}"
    mode: "0700"
  loop: "{{ kubeconfig_targets }}"
  when:
    - kubeconfig_targets is defined
    - kubeconfig_targets | length > 0

- name: Ensure kubeconfig file ownership and mode
  ansible.builtin.file:
    path: "{{ item.path }}"
    state: file
    owner: "{{ item.owner }}"
    group: "{{ item.owner }}"
    mode: "0600"
  loop: "{{ kubeconfig_targets }}"
  when:
    - kubeconfig_targets is defined
    - kubeconfig_targets | length > 0

- name: Wait for Kubernetes API server to be fully ready (RBAC reconciled)
  ansible.builtin.command: kubectl get nodes
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: k8s_api_ready
  changed_when: false
  until: k8s_api_ready.rc == 0
  retries: 12
  delay: 10

- name: Install Node Feature Discovery (NFD)
  ansible.builtin.command: kubectl apply -k {{ nfd_kustomize_ref }}
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: nfd_apply
  changed_when: "'created' in nfd_apply.stdout or 'configured' in nfd_apply.stdout"
  run_once: true
  when: k8s_node_info_enabled | default(true)
  tags:
    - addon
    - node_info
    - k9s
    - nfd



- name: Wait for NFD master deployment to be available
  ansible.builtin.command: >-
    kubectl -n {{ nfd_namespace }} rollout status deployment/{{ nfd_master_deployment_name }} --timeout=300s
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: nfd_master_rollout
  changed_when: false
  until: nfd_master_rollout.rc == 0
  retries: 6
  delay: 10
  run_once: true
  when: k8s_node_info_enabled | default(true)
  tags:
    - addon
    - node_info
    - k9s
    - nfd

- name: Wait for NFD worker daemonset rollout
  ansible.builtin.command: >-
    kubectl -n {{ nfd_namespace }} rollout status daemonset/{{ nfd_worker_daemonset_name }} --timeout=600s
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: nfd_worker_rollout
  changed_when: false
  until: nfd_worker_rollout.rc == 0
  retries: 10
  delay: 10
  run_once: true
  when: k8s_node_info_enabled | default(true)
  tags:
    - addon
    - node_info
    - k9s
    - nfd

- name: Get NFD node labels overview
  ansible.builtin.command: >-
    kubectl get nodes -o custom-columns=NAME:.metadata.name,NFD_LABELS:.metadata.labels.feature\.node\.kubernetes\.io/system-os_release.ID
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: nfd_node_labels
  changed_when: false
  failed_when: false
  run_once: true
  when: k8s_node_info_enabled | default(true)
  tags:
    - addon
    - node_info
    - k9s
    - nfd

- name: Display NFD node labels overview
  ansible.builtin.debug:
    var: nfd_node_labels.stdout_lines
  run_once: true
  when:
    - k8s_node_info_enabled | default(true)
    - nfd_node_labels is defined
  tags:
    - addon
    - node_info
    - k9s
    - nfd

- name: Get cluster node status
  ansible.builtin.command: kubectl get nodes
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: node_status
  changed_when: false
  ignore_errors: true

- name: Display cluster node status
  ansible.builtin.debug:
    var: node_status.stdout_lines

- name: Get cluster node status (user kubeconfig)
  ansible.builtin.command: kubectl get nodes
  environment:
    KUBECONFIG: "{{ ansible_user_kubeconfig_path | default(kubeconfig_path) }}"
  register: node_status_user
  changed_when: false
  ignore_errors: true
  become: true
  become_user: "{{ ansible_user | default(kubeconfig_user) }}"

- name: Display cluster node status (user kubeconfig)
  ansible.builtin.debug:
    var: node_status_user.stdout_lines

- name: Verify control plane node is Ready
  ansible.builtin.command: kubectl get nodes -l node-role.kubernetes.io/control-plane -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}'
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: control_plane_ready
  changed_when: false
  ignore_errors: true

- name: Assert control plane is Ready
  ansible.builtin.assert:
    that:
      - "'True' in control_plane_ready.stdout"
    success_msg: "Control plane node is Ready"
    fail_msg: "Control plane node is not Ready"
  when: control_plane_ready.stdout is defined

- name: Display control plane initialization summary
  ansible.builtin.debug:
    msg:
      - "=== Control Plane Initialization Complete ==="
      - "Kubernetes API: Ready"
      - "CNI: Not installed (manage manually via calico_bgp_manage.yaml)"
      - "Next step: Run kuber_worker_join.yaml to join workers"
      - "Next step: Run calico_bgp_manage.yaml to configure Calico CNI"
      - "Verification: Run kuber_verify.yaml for full cluster health check"
