---
- name: Check if Kubernetes is already initialized
  ansible.builtin.stat:
    path: /etc/kubernetes/admin.conf
  register: k8s_admin_conf

- name: Set initialized flag
  ansible.builtin.set_fact:
    kubeadm_cluster_initialized: "{{ k8s_admin_conf.stat.exists }}"

- name: Override kubeconfig user when root
  ansible.builtin.set_fact:
    kubeconfig_user: "{{ ansible_user }}"
  when:
    - kubeconfig_user == 'root'
    - ansible_user is defined
    - ansible_user != 'root'

- name: Fail if Kubernetes is already initialized
  ansible.builtin.fail:
    msg: "Kubernetes cluster is already initialized. Use kuber_plane_reset.yaml to reset first."
  when:
    - kubeadm_cluster_initialized
    - not (kubeadm_refresh_on_existing | default(true))
    - ansible_run_tags is not defined or 'cni' not in ansible_run_tags

- name: Create containerd config directory
  ansible.builtin.file:
    path: /etc/containerd
    state: directory
    mode: "0755"

- name: Configure containerd cgroup driver
  ansible.builtin.template:
    src: "{{ playbook_dir }}/roles/kuber/templates/containerd-config.toml.j2"
    dest: /etc/containerd/config.toml
    mode: "0644"

- name: Restart containerd
  ansible.builtin.systemd:
    name: containerd
    state: restarted
    enabled: true

- name: Create kubelet systemd service file
  ansible.builtin.copy:
    content: |
      [Unit]
      Description=kubelet: The Kubernetes Node Agent
      Documentation=https://kubernetes.io/docs/
      After=containerd.service
      Requires=containerd.service

      [Service]
      ExecStart=/usr/local/bin/kubelet
      Restart=always
      StartLimitInterval=0
      RestartSec=10

      [Install]
      WantedBy=multi-user.target
    dest: /etc/systemd/system/kubelet.service
    mode: "0644"

- name: Create kubelet drop-in directory
  ansible.builtin.file:
    path: /etc/systemd/system/kubelet.service.d
    state: directory
    mode: "0755"

- name: Create kubeadm kubelet drop-in configuration
  ansible.builtin.copy:
    content: |
      [Service]
      Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
      Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
      # This is a file that "kubeadm init" and "kubeadm join" generate at runtime
      EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
      # This is a file that the user can use for overrides of the kubelet args
      EnvironmentFile=-/etc/default/kubelet
      ExecStart=
      ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
    dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
    mode: "0644"

- name: Reload systemd daemon
  ansible.builtin.systemd:
    daemon_reload: true

- name: Stop kubelet service
  ansible.builtin.systemd:
    name: kubelet
    state: stopped

- name: Enable kubelet service
  ansible.builtin.systemd:
    name: kubelet
    enabled: true

- name: Copy kubeadm config
  ansible.builtin.template:
    src: kubeadm-config.yaml.j2
    dest: /tmp/kubeadm-config.yaml
    mode: "0644"

- name: Refresh control plane certificates and admin kubeconfig
  ansible.builtin.command: "{{ item }}"
  loop:
    - kubeadm init phase certs apiserver --config=/tmp/kubeadm-config.yaml
    - kubeadm init phase kubeconfig admin --config=/tmp/kubeadm-config.yaml
  register: kubeadm_refresh
  changed_when: true
  when:
    - kubeadm_cluster_initialized
    - kubeadm_refresh_on_existing | default(true)
    - ansible_run_tags is not defined or 'cni' not in ansible_run_tags

- name: Check if Kubernetes API port is in use
  ansible.builtin.command: ss -ltnp
  register: k8s_api_port_check
  changed_when: false
  failed_when: false
  when:
    - not kubeadm_cluster_initialized
    - ansible_run_tags is not defined or 'cni' not in ansible_run_tags

- name: Fail if Kubernetes API port is already in use
  ansible.builtin.fail:
    msg:
      - "Kubernetes API port {{ vault_k8s_api_port }} is already in use"
      - "Stop the service using the port or run kuber_plane_reset.yaml before init"
  when:
    - not kubeadm_cluster_initialized
    - k8s_api_port_check.stdout is search(':' ~ vault_k8s_api_port)
    - ansible_run_tags is not defined or 'cni' not in ansible_run_tags

- name: Initialize Kubernetes control plane
  ansible.builtin.command: kubeadm init --config=/tmp/kubeadm-config.yaml
  register: kubeadm_init
  changed_when: true
  when:
    - ansible_run_tags is not defined or 'cni' not in ansible_run_tags
    - not kubeadm_cluster_initialized

- name: Look up kubeconfig user home directory
  ansible.builtin.getent:
    database: passwd
    key: "{{ kubeconfig_user }}"
  register: kubeconfig_user_passwd

- name: Look up ansible user home directory
  ansible.builtin.getent:
    database: passwd
    key: "{{ ansible_user }}"
  register: ansible_user_passwd
  when:
    - ansible_user is defined
    - ansible_user != 'root'
    - ansible_user != kubeconfig_user

- name: Set kubeconfig user home and path
  ansible.builtin.set_fact:
    kubeconfig_user_home: >-
      {{
        (kubeconfig_user_passwd.ansible_facts.getent_passwd[kubeconfig_user][5]
        | default(kubeconfig_user_home))
      }}
    kubeconfig_path: "{{ kubeconfig_user_home }}/.kube/config"

- name: Set ansible user kubeconfig path
  ansible.builtin.set_fact:
    ansible_user_kubeconfig_path: >-
      {{
        (ansible_user_passwd.ansible_facts.getent_passwd[ansible_user][5]
        | default('/home/' ~ ansible_user))
      }}/.kube/config
  when:
    - ansible_user is defined
    - ansible_user != 'root'

- name: Build kubeconfig target list
  ansible.builtin.set_fact:
    kubeconfig_targets: >-
      {{
        (
          ([{'path': kubeconfig_path, 'owner': kubeconfig_user}]
          if (kubeconfig_user == 'root' or kubeconfig_path is not match('^/root/'))
          else [])
          +
          ([{'path': ansible_user_kubeconfig_path, 'owner': ansible_user}]
          if (ansible_user_kubeconfig_path is defined and ansible_user_kubeconfig_path | length > 0)
          else [])
        )
      }}

- name: Create .kube directory
  ansible.builtin.file:
    path: "{{ item.path | dirname }}"
    state: directory
    mode: "0700"
    owner: "{{ item.owner }}"
    group: "{{ item.owner }}"
  loop: "{{ kubeconfig_targets }}"
  when:
    - kubeconfig_targets is defined
    - kubeconfig_targets | length > 0

- name: Copy admin.conf to user's kubeconfig
  ansible.builtin.copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ item.path }}"
    remote_src: true
    mode: "0600"
    owner: "{{ item.owner }}"
    group: "{{ item.owner }}"
  loop: "{{ kubeconfig_targets }}"
  when:
    - kubeconfig_targets is defined
    - kubeconfig_targets | length > 0

- name: Ensure kubeconfig directory ownership and mode
  ansible.builtin.file:
    path: "{{ item.path | dirname }}"
    state: directory
    owner: "{{ item.owner }}"
    group: "{{ item.owner }}"
    mode: "0700"
  loop: "{{ kubeconfig_targets }}"
  when:
    - kubeconfig_targets is defined
    - kubeconfig_targets | length > 0

- name: Ensure kubeconfig file ownership and mode
  ansible.builtin.file:
    path: "{{ item.path }}"
    state: file
    owner: "{{ item.owner }}"
    group: "{{ item.owner }}"
    mode: "0600"
  loop: "{{ kubeconfig_targets }}"
  when:
    - kubeconfig_targets is defined
    - kubeconfig_targets | length > 0

- name: Install Calico Tigera Operator
  ansible.builtin.command: kubectl apply -f {{ calico_tigera_operator_url }}
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: calico_operator
  changed_when: "'created' in calico_operator.stdout or 'configured' in calico_operator.stdout"
  until: calico_operator.rc == 0
  retries: 3
  delay: 5
  tags:
    - cni
    - calico

- name: Wait for Tigera Operator to be ready
  ansible.builtin.command: kubectl wait --for=condition=ready pod -l name=tigera-operator -n tigera-operator --timeout=300s
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: operator_wait
  changed_when: false
  until: operator_wait.rc == 0
  retries: 10
  delay: 10
  tags:
    - cni
    - calico

- name: Fail if Calico vault variables are missing
  ansible.builtin.assert:
    that:
      - vault_ipPools is defined
      - vault_ipPools | length > 0
      - vault_ipPools_encapsulation is defined
      - vault_ipPools_encapsulation | length > 0
      - vault_ipPools_encapsulation is not match('^\[.*\]$')
      - vault_ipPools_cidr is defined
      - vault_ipPools_cidr | length > 0
      - vault_ipPools_cidr is not match('^\[.*\]$')
      - vault_ipPools_blockSize is defined
      - natOutgoing is defined
      - nodeSelector is defined
      - nodeSelector | length > 0
      - nodeSelector is not match('^\[.*\]$')
      - mtu is defined
      - vault_interface is defined
      - vault_interface | length > 0
      - vault_interface is not match('^\[.*\]$')
    fail_msg: >-
      Calico CNI vault variables are missing or placeholders. Set vault_ipPools*,
      natOutgoing, nodeSelector, mtu, and vault_interface in vault_secrets.yml to
      avoid applying defaults.
    success_msg: "Calico CNI vault variables are defined"
  tags:
    - cni
    - calico

- name: Fail if vault admin user is missing
  ansible.builtin.assert:
    that:
      - vault_admin_user is defined
      - vault_admin_user | length > 0
      - vault_admin_user is not match('^\[.*\]$')
    fail_msg: >-
      vault_admin_user is missing or a placeholder. Set vault_admin_user in
      vault_secrets.yml to ensure kubeconfig is written to the correct user.
    success_msg: "vault_admin_user is defined"

- name: Generate Calico custom resources from template
  ansible.builtin.template:
    src: calico-custom-resources.yaml.j2
    dest: /tmp/calico-custom-resources.yaml
    mode: "0644"
  tags:
    - cni
    - calico

- name: Apply Calico custom resources
  ansible.builtin.command: kubectl apply -f /tmp/calico-custom-resources.yaml
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: calico_custom_apply
  changed_when: "'created' in calico_custom_apply.stdout or 'configured' in calico_custom_apply.stdout"
  tags:
    - cni
    - calico

- name: Detect Calico node pods (k8s-app label)
  ansible.builtin.command: kubectl get pods -A -l k8s-app=calico-node -o name
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: calico_node_k8s_label
  changed_when: false
  failed_when: false
  until: calico_node_k8s_label.stdout | length > 0
  retries: 6
  delay: 10
  tags:
    - cni
    - calico

- name: Detect Calico node pods (app label)
  ansible.builtin.command: kubectl get pods -A -l app=calico-node -o name
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: calico_node_app_label
  changed_when: false
  failed_when: false
  until: calico_node_app_label.stdout | length > 0
  retries: 6
  delay: 10
  when: calico_node_k8s_label.stdout | length == 0
  tags:
    - cni
    - calico

- name: Set Calico node label selector
  ansible.builtin.set_fact:
    calico_node_label_selector: >-
      {{ 'k8s-app=calico-node'
      if (calico_node_k8s_label.stdout | length > 0)
      else 'app=calico-node' }}
  when:
    - calico_node_k8s_label.stdout | length > 0 or
      (calico_node_app_label.stdout | default('') | length > 0)
  tags:
    - cni
    - calico

- name: Fail if Calico node pods not found
  ansible.builtin.fail:
    msg: >-
      Calico node pods not found with labels k8s-app=calico-node or app=calico-node.
  when:
    - calico_node_k8s_label.stdout | length == 0
    - calico_node_app_label.stdout | default('') | length == 0
  tags:
    - cni
    - calico

- name: Wait for Calico to be ready
  ansible.builtin.command: >-
    kubectl wait --for=condition=ready pod -A -l {{ calico_node_label_selector }} --timeout=600s
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: calico_wait
  changed_when: false
  until: calico_wait.rc == 0
  retries: 20
  delay: 10
  when: calico_node_label_selector is defined
  tags:
    - cni
    - calico

- name: Get cluster node status
  ansible.builtin.command: kubectl get nodes
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: node_status
  changed_when: false
  ignore_errors: true

- name: Display cluster node status
  ansible.builtin.debug:
    var: node_status.stdout_lines

- name: Get cluster node status (user kubeconfig)
  ansible.builtin.command: kubectl get nodes
  environment:
    KUBECONFIG: "{{ ansible_user_kubeconfig_path | default(kubeconfig_path) }}"
  register: node_status_user
  changed_when: false
  ignore_errors: true
  become: true
  become_user: "{{ ansible_user | default(kubeconfig_user) }}"

- name: Display cluster node status (user kubeconfig)
  ansible.builtin.debug:
    var: node_status_user.stdout_lines

- name: Verify control plane node is Ready
  ansible.builtin.command: kubectl get nodes -l node-role.kubernetes.io/control-plane -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}'
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: control_plane_ready
  changed_when: false
  ignore_errors: true

- name: Assert control plane is Ready
  ansible.builtin.assert:
    that:
      - "'True' in control_plane_ready.stdout"
    success_msg: "Control plane node is Ready"
    fail_msg: "Control plane node is not Ready"
  when: control_plane_ready.stdout is defined

- name: Get Calico node pods
  ansible.builtin.command: kubectl get pods -A -l {{ calico_node_label_selector }}
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: calico_pods
  changed_when: false
  when: calico_node_label_selector is defined
  tags:
    - cni
    - calico

- name: Display Calico pods
  ansible.builtin.debug:
    var: calico_pods.stdout_lines
  tags:
    - cni
    - calico

- name: Get Tigera Operator pods
  ansible.builtin.command: kubectl get pods -n tigera-operator
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: tigera_pods
  changed_when: false
  tags:
    - cni
    - calico

- name: Display Tigera Operator pods
  ansible.builtin.debug:
    var: tigera_pods.stdout_lines
  tags:
    - cni
    - calico

- name: Verify Tigera Operator is Running
  ansible.builtin.command: kubectl get pods -n tigera-operator -l name=tigera-operator -o jsonpath='{.items[*].status.phase}'
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: tigera_phase
  changed_when: false
  ignore_errors: true
  tags:
    - cni
    - calico

- name: Assert Tigera Operator is Running
  ansible.builtin.assert:
    that:
      - "'Running' in tigera_phase.stdout"
    success_msg: "Tigera Operator is Running"
    fail_msg: "Tigera Operator is not Running"
  when: tigera_phase.stdout is defined
  tags:
    - cni
    - calico

- name: Wait for Calico IPPool CRD to be established
  ansible.builtin.command: kubectl wait --for=condition=Established crd/ippools.crd.projectcalico.org --timeout=120s
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: ippool_crd_wait
  changed_when: false
  failed_when: false
  until: ippool_crd_wait.rc == 0
  retries: 6
  delay: 10
  tags:
    - cni
    - calico

- name: Get Calico IP pools
  ansible.builtin.command: kubectl get ippool -o wide
  environment:
    KUBECONFIG: /etc/kubernetes/admin.conf
  register: ippool_status
  changed_when: false
  failed_when: false
  until: ippool_status.rc == 0
  retries: 6
  delay: 10
  tags:
    - cni
    - calico

- name: Display Calico IP pools
  ansible.builtin.debug:
    var: ippool_status.stdout_lines
  when: ippool_status.rc == 0
  tags:
    - cni
    - calico

- name: Display control plane initialization summary
  ansible.builtin.debug:
    msg:
      - "=== Control Plane Initialization Complete ==="
      - "Kubernetes API: Ready"
      - "Calico CNI: Installed"
      - "Tigera Operator: Running"
      - "Next step: Run kuber_worker_join.yaml to join workers"
      - "Verification: Run kuber_verify.yaml for full cluster health check"
