---
# Example Ansible Vault secrets file for libssh connection
# This file should be encrypted with ansible-vault

# SSH key passphrase (for ~/.ssh/id_rsa)
vault_ssh_key_passphrase: "[your-ssh-key-passphrase-here]"

# Sudo/become password for workers_all
vault_become_pass: "[your-password-here]"

# Admin user for kubeconfig ownership
vault_admin_user: "[your-username]"

# Reserve SSH ports
vault_ssh_one: "[custom-ssh-port-1]"
vault_ssh_two: "[custom-ssh-port-2]"

# DNS Server Configuration
vault_dns_zone: "[dns-zone]"

vault_dns_servers:
  - "[primary-dns-hostname]"
  - "[secondary-dns-hostname]"

dns_upstrea_dns:
  - "[upstream-dns-1]"
  - "[upstream-dns-2]"

vault_dns_server_primary: "[primary-dns-server-ip]"
vault_dns_server_secondary: "[secondary-dns-server-ip]"

vault_dns_client_options:
  - "timeout:2"
  - "attempts:3"
  - "rotate"

# Kubernetes nodes: node-local DNS forwarder (dnsmasq) for IPv4-only environments
#
# If your upstream DNS returns AAAA records but your nodes have no IPv6 default route,
# containerd pulls and ACME (Let's Encrypt) may try IPv6 first and fail.
# Enable node-local dnsmasq to filter AAAA and forward to the real DNS servers.
#
# vault_dns_client_node_local_enabled: true
# vault_dns_client_filter_aaaa: true

# cert-manager (recommended TLS approach for Kubernetes)
#
# When using cert-manager with Traefik, Traefik does not need to run its own ACME resolver
# and does not need to persist /data/acme.json.
#
# vault_cert_manager_enabled: true
# vault_cert_manager_acme_email: "[your-email]"
# vault_cert_manager_clusterissuer_prod_enabled: true

vault_dns_records:
  # Kubernetes cluster DNS names (PRIMARY - use these for cluster)
  # API endpoint
  - type: "A"
    name: "k8s-api"
    ip: "[vip-address]"

  # Control plane nodes
  - type: "A"
    name: "k8s-plane1"
    ip: "[plane1-ip]"
  - type: "A"
    name: "k8s-plane2"
    ip: "[plane2-ip]"

  # Worker nodes
  - type: "A"
    name: "k8s-worker1"
    ip: "[worker1-ip]"
  - type: "A"
    name: "k8s-worker2"
    ip: "[worker2-ip]"

  # Legacy system hostnames (KEEP for compatibility/fallback)
  - type: "A"
    name: "[plane1-hostname]"
    ip: "[plane1-ip]"
  - type: "A"
    name: "[plane2-hostname]"
    ip: "[plane2-ip]"
  - type: "A"
    name: "[worker1-hostname]"
    ip: "[worker1-ip]"
  - type: "A"
    name: "[worker2-hostname]"
    ip: "[worker2-ip]"

  # Kubernetes services (external ingress/LoadBalancer)
  - type: "A"
    name: "[ingress-hostname]"
    ip: "[ingress-ip]"
  - type: "A"
    name: "[longhorn-hostname]"
    ip: "[longhorn-ip]"

  # Infrastructure services
  - type: "A"
    name: "[nfs-hostname]"
    ip: "[nfs-ip]"
  - type: "A"
    name: "vip"
    ip: "[vip-address]"

# DNS configuration
vault_dns_allowed_networks:
  # Add all networks that should be able to query DNS
  # This list supports multiple networks for flexible deployment
  # - Always include localhost for local queries
  - "[physical-network-cidr]"
  - "[vpn-network-cidr]"
  - "[cloud-network-cidr]"
  - "[loopback-cidr]"

# Connection settings for DNS servers
vault_dns_ansible_user: "[username]"
vault_dns_ansible_port: "[ssh-port]"
vault_dns_ssh_key_path: "[/path/to/key]"

# WireGuard Configuration
vault_wg_interface: "[interface-name]"
vault_wg_server_port: "[server-port]"
vault_wg_client_default_port: "[client-default-port]"
vault_wg_client_port_start: "[port-range-start]"
vault_wg_client_port_end: "[port-range-end]"
vault_wg_network_cidr: "[vpn-network-cidr]"
vault_wg_server_ip: "[server-vpn-ip]"
vault_wg_dns_primary: "[dns-server-ip]"

# WireGuard Server Keys (auto-generated, store here after first deployment)
vault_wg_server_private_key: null
vault_wg_server_public_key: null

# WireGuard Peers Configuration
# Each peer maps to a host group in inventory
#
# Required fields:
# - name: Unique identifier (must match vault_wg_peer_private_keys and vault_wg_peer_public_keys)
# - host_group: Maps to Ansible inventory group name
# - allowed_ips: VPN IP address for this peer (used in AllowedIPs)
# - endpoint: Public or internal IP:PORT for connecting
#
# Optional fields:
# - is_server: true for server nodes that route pod network traffic
#              When true, vault_wg_routed_cidrs (e.g., Calico pod CIDR) is added
#              to AllowedIPs in client configs. Set for control planes, HAProxy, BGP routers.
#              NOTE: any tasks that transform/rebuild items in vault_wg_peers MUST preserve
#              optional keys like is_server. If is_server is dropped, templates will not append
#              vault_wg_routed_cidrs and routed networks will be unreachable even if handshakes work.
# - client_listen_port: UDP port for incoming WireGuard connections (for NAT peers)
# - dns_name: DNS name for this peer (for reference only, IPs used for endpoints)
#
# Example multi-server mesh with pod routing:
vault_wg_peers:
  # Servers (is_server: true) - route pod network traffic
  - name: "[haproxy-peer-name]"
    host_group: "[haproxy-host-group]"
    is_server: true
    allowed_ips: "[haproxy-vpn-ip]/32"
    endpoint: "[haproxy-public-ip]:[port]"
    dns_name: "haproxy.cluster.local"

  - name: "[bgp-peer-name]"
    host_group: "[bgp-host-group]"
    is_server: true
    allowed_ips: "[bgp-vpn-ip]/32"
    endpoint: "[bgp-internal-ip]:[port]"
    dns_name: "bgp.cluster.local"

  - name: "[control-plane-peer-name]"
    host_group: "[control-plane-host-group]"
    is_server: true
    allowed_ips: "[control-plane-vpn-ip]/32"
    endpoint: "[control-plane-internal-ip]:[port]"
    dns_name: "k8s-plane1.cluster.local"

  # Clients (no is_server) - workers, external nodes
  - name: "[worker-peer-name]"
    host_group: "[worker-host-group]"
    allowed_ips: "[worker-vpn-ip]/32"
    endpoint: "[worker-public-ip]:[port]"
    client_listen_port: "[unique-port-for-nat-peers]"
    dns_name: "k8s-worker1.cluster.local"

# WireGuard Server IPs (multi-server mesh - maps inventory group to VPN IP)
vault_wg_server_ips:
  haproxy_hostname: "[haproxy-vpn-ip]"
  bgp_hostname: "[bgp-vpn-ip]"
  control_plane_hostname: "[control-plane-vpn-ip]"

# WireGuard Server Ports (multi-server mesh - maps inventory group to port)
vault_wg_server_ports:
  haproxy_hostname: "[haproxy-wg-port]"
  bgp_hostname: "[bgp-wg-port]"
  control_plane_hostname: "[plane-wg-port]"

# WireGuard Peer Keys (populated after key generation)
# Each node (server or client) has its own key pair
vault_wg_peer_private_keys:
  haproxy_hostname: "[haproxy-private-key]"
  bgp_hostname: "[bgp-private-key]"
  control_plane_hostname: "[plane-private-key]"
  worker_hostname: "[worker-private-key]"

vault_wg_peer_public_keys:
  haproxy_hostname: "[haproxy-public-key]"
  bgp_hostname: "[bgp-public-key]"
  control_plane_hostname: "[plane-public-key]"
  worker_hostname: "[worker-public-key]"

# WireGuard Firewall Configuration
vault_wg_allowed_networks:
  - "[physical-network-cidr-1]"
  - "[vpn-network-cidr]"
  - "[physical-network-cidr-2]"
  - "[physical-network-cidr-3]"
  - "[loopback-cidr]"

# WireGuard Routed CIDRs
# Networks routed through WireGuard server peers (is_server: true)
# These CIDRs are automatically added to AllowedIPs for server peers
# Auto-populated by wireguard_manage.yaml from:
#   - vault_k8s_api_vip/32 (Kubernetes API VIP for HA)
#   - vault_metallb_pool_cidr (MetalLB LoadBalancer subnet)
#   - vault_db_wg_route_cidr (DB WG endpoint route for SNAT pod -> DB access)
# You typically don't need to set this manually
vault_wg_routed_cidrs:
  - "[pod-network-cidr]"
  - "[k8s-api-vip]/32"

# Optional explicit DB WG route for worker/plane peers.
# Used by wireguard_manage.yaml to add a direct WG route to DB endpoint.
# Keep as /32 for single DB host.
vault_db_wg_route_cidr: "[db-wg-ip]/32"

# HAProxy Kubernetes API Configuration
# Used for external access to Kubernetes API (if needed)
vault_haproxy_k8s_frontend_port: "[haproxy-frontend-port]"
vault_haproxy_k8s_backend_port: "[haproxy-backend-port]"
vault_haproxy_k8s_backend_ip: "[control-plane-ip]"

# HAProxy Ingress Configuration (HTTP/HTTPS to Kubernetes via BGP VIP)
# Enables external internet access to Kubernetes services through Traefik ingress
#
# Traffic flow:
#   Internet -> HAProxy (public IP) -> BGP VIP -> FRR -> MetalLB -> Traefik -> App
#
# Set vault_haproxy_ingress_enabled: true to activate ingress forwarding
vault_haproxy_ingress_enabled: false
vault_haproxy_ingress_http_port: 80
vault_haproxy_ingress_https_port: 443
vault_haproxy_ingress_backend_ip: "[bgp-vip-address]" # BGP VIP (same as vault_bgp_keepalived_vip)
vault_haproxy_ingress_backend_http_port: 80
vault_haproxy_ingress_backend_https_port: 443

# Keepalived VIP Configuration
# Simplified architecture: VIP floats between control planes via VRRP
# Workers connect to VIP -> traffic goes directly to kube-apiserver on MASTER node
# No HAProxy or DNAT required for internal k8s API HA
vault_keepalived_vip: "[vip-address]"
vault_keepalived_vip_cidr: "32"
vault_keepalived_vip_interface: "[vip-interface]"
vault_keepalived_password: "[keepalived-password]"
vault_keepalived_router_id: "51"
vault_keepalived_script_user: "[admin-username]"

# Kubernetes Version (use specific version to prevent accidental upgrades)
vault_kubernetes_version: "1.35"

# Kubernetes API VIP (for workers to join)
# This VIP is managed by Keepalived and floats between control planes
vault_k8s_api_vip: "[vip-address]"
vault_k8s_api_port: "[k8s-api-port]"

# Kubernetes DNS configuration (NEW - for DNS-based cluster communication)
# Use these DNS names instead of IPs for cluster communication
# Set vault_k8s_use_dns_endpoint: true to enable DNS-based configuration
vault_k8s_dns_prefix: "k8s"
vault_k8s_api_dns: "k8s-api.{{ vault_dns_zone }}"
vault_k8s_api_dns_name: "k8s-api"
vault_k8s_use_dns_endpoint: true

# Control Plane Configuration (multi-plane HA support)
# Add entries here for each control plane node
# - name: inventory hostname
# - dns_name: DNS name for Kubernetes cluster (NEW)
# - wireguard_ip: node's WireGuard IP address
# - backend_port: Backend port (if using HAProxy)
# - api_port: kube-apiserver port (use [k8s-api-port])
# - priority: VRRP priority (higher = preferred MASTER, e.g., 150, 140, 130)
#
# Example for 3-plane HA cluster:
# vault_k8s_control_planes:
#   - name: "[control-plane-hostname-1]"
#     dns_name: "k8s-plane1"
#     wireguard_ip: "[control-plane-wg-ip-1]"
#     api_port: "[k8s-api-port]"
#     priority: 150
#   - name: "[control-plane-hostname-2]"
#     dns_name: "k8s-plane2"
#     wireguard_ip: "[control-plane-wg-ip-2]"
#     api_port: "[k8s-api-port]"
#     priority: 140
#   - name: "[control-plane-hostname-3]"
#     dns_name: "k8s-plane3"
#     wireguard_ip: "[control-plane-wg-ip-3]"
#     api_port: "[k8s-api-port]"
#     priority: 130
vault_k8s_control_planes:
  - name: "[control-plane-hostname]"
    dns_name: "k8s-plane1"
    wireguard_ip: "[control-plane-wg-ip]"
    backend_port: "[k8s-api-port]"
    api_port: "[k8s-api-port]"
    priority: 150

# Worker Configuration (NEW - for DNS-based worker configuration)
# Add entries here for each worker node
# - name: inventory hostname
# - dns_name: DNS name for Kubernetes cluster
# - wireguard_ip: node's WireGuard IP address
# - backend_port: Backend port (if using HAProxy)
# - api_port: Kubernetes API server port
#
# Example:
# vault_k8s_workers:
#   - name: "[worker-hostname-1]"
#     dns_name: "k8s-worker1"
#     wireguard_ip: "[worker-wg-ip-1]"
#     backend_port: "[k8s-api-port]"
#     api_port: "[k8s-api-port]"
#   - name: "[worker-hostname-2]"
#     dns_name: "k8s-worker2"
#     wireguard_ip: "[worker-wg-ip-2]"
#     backend_port: "[k8s-api-port]"
#     api_port: "[k8s-api-port]"
vault_k8s_workers:
  - name: "[worker-hostname]"
    dns_name: "k8s-worker1"
    wireguard_ip: "[worker-wg-ip]"
    backend_port: "[k8s-api-port]"
    api_port: "[k8s-api-port]"

# Kubernetes Network Configuration
vault_k8s_pod_subnet: "[pod-network-cidr]"
vault_k8s_service_subnet: "[service-network-cidr]"

# Kubernetes Pre-flight Validation Configuration
vault_k8s_preflight_skip_docker_check: false
vault_k8s_preflight_skip_swap_check: false
vault_k8s_preflight_skip_port_check: false
vault_k8s_preflight_skip_cni_check: false
vault_k8s_preflight_skip_container_runtime_check: false
vault_k8s_preflight_skip_process_check: false
vault_k8s_preflight_fail_on_warnings: false

# Calico CNI Configuration
# Version of Calico to install (matches Tigera operator manifests)
vault_calico_version: "v3.31.3"

# Calico node IP autodetection interface
vault_interface: "[interface-name]"

# IP Pool name
vault_ipPools: "default-ipv4-ippool"

# Pod CIDR for IP pool (should match vault_k8s_pod_subnet)
vault_ipPools_cidr: "[pod-network-cidr]"

# Encapsulation mode for pod traffic between nodes
# Options: "VXLANCrossSubnet", "VXLAN", "IPIPCrossSubnet", "IPIP", "None"
# - IPIP: Always use IPIP encapsulation (REQUIRED for WireGuard networks)
# - IPIPCrossSubnet: IPIP only for cross-subnet traffic
# - VXLANCrossSubnet: VXLAN only for cross-subnet traffic
# - VXLAN: Always use VXLAN encapsulation
# - None: No encapsulation (requires L2 connectivity between nodes)
vault_ipPools_encapsulation: "IPIP"

# Enable NAT for pod traffic leaving the cluster
# CRITICAL for WireGuard: Must be true so pods can reach node WireGuard IPs
# Without this, pods cannot reach the Kubernetes API or other nodes
natOutgoing: true

# Node selector for IP pool (which nodes can use this pool)
# "all()" means all nodes can use this IP pool
nodeSelector: "all()"

# Block size for per-node pod CIDR allocation (default /26 = 64 IPs per node)
vault_ipPools_blockSize: 26

# MTU for Calico network interfaces (adjust for encapsulation overhead)
# Calculation for WireGuard + IPIP:
#   WireGuard MTU: 1420 (typical)
#   IPIP overhead: 20 bytes
#   Safety margin: 20 bytes
#   Recommended: 1380 for WireGuard + IPIP
#
# Standard values:
# - 1440 for VXLAN (1500 - 60 bytes overhead)
# - 1480 for IPIP on standard networks (1500 - 20 bytes overhead)
# - 1380 for IPIP over WireGuard (1420 - 20 - 20)
mtu: 1380

# Calico Typha Configuration
# Typha is a fan-out proxy that sits between Kubernetes API and Felix
# It reduces API server load by caching and distributing updates
#
# vault_calico_typha_replicas: Number of Typha replicas
# - For small clusters (< 5 nodes): 1 is sufficient
# - For larger clusters: 2-3 for high availability
# - The Tigera operator autoscaler may adjust this automatically
# - Anti-affinity rules prevent multiple Typha pods on same node (port conflict)
vault_calico_typha_replicas: 1

# Calico BGP Tuning (for MetalLB BGP)
#
# Calico's calico-node runs BIRD and binds TCP/179 by default.
# MetalLB BGP also requires TCP/179. To avoid a port conflict, move Calico BGP
# to a non-standard port (e.g., 178) and restart calico-node.
vault_calico_bgp_enabled: true
vault_calico_bgp_listen_port: 178
vault_calico_bgp_restart_calico_node: true

# MetalLB LoadBalancer Configuration (BGP over WireGuard)
#
# Best practice for WireGuard/L3 clusters:
# - Keep Calico encapsulation as IPIP
# - Use MetalLB in BGP mode to announce Service LoadBalancer IPs
# - Peer MetalLB speakers (k8s nodes) to a dedicated BGP router host (e.g., bay_bgp)
# - Use a dedicated LB pool CIDR/range (DO NOT reuse the WireGuard interface subnet)

vault_metallb_enabled: true
vault_metallb_version: "v0.14.9"
vault_metallb_mode: "bgp" # bgp or layer2

# Dedicated address pool for LoadBalancer services
vault_metallb_pool_name: "wg-lb-pool"
vault_metallb_pool_cidr: "[metallb-pool-cidr]/24"

# MetalLB BGP configuration
vault_metallb_bgp_my_asn: "[metallb-my-asn]"

# For single BGP router:
# vault_metallb_bgp_peers:
#   - name: "bgp-router-1"
#     peer_address: "[bgp-router-1-wg-ip]"
#     peer_asn: "[router-asn]"

# For BGP HA (two routers with VRRP):
# MetalLB peers with BOTH router IPs (not the VIP) for redundancy
vault_metallb_bgp_peers:
  - name: "bgp-router-1"
    peer_address: "[bgp-router-1-wg-ip]"
    peer_asn: "[router-asn]"
  - name: "bgp-router-2"
    peer_address: "[bgp-router-2-wg-ip]"
    peer_asn: "[router-asn]"

# Advertisements
# Keep per-service /32 advertisements enabled; optionally also advertise an aggregate prefix
vault_metallb_bgp_advertise_per_service: true
vault_metallb_bgp_aggregate_enabled: true
vault_metallb_bgp_aggregate_length_v4: 24

# Optional smoke test (creates a temporary LB service)
vault_metallb_run_smoke_test: false

# BGP Router (FRR) Configuration (e.g., bay_bgp)
#
# The BGP router accepts routes for the MetalLB pool from Kubernetes nodes and forwards traffic.
# Neighbors should be the WireGuard IPs of your Kubernetes nodes (planes/workers running MetalLB speaker).
vault_bgp_router_asn: "[router-asn]"
vault_bgp_router_router_id: "[bgp-router-1-wg-ip]"
vault_bgp_router_listen_interface: "[wg-interface]"
vault_bgp_router_update_source: "[wg-interface]"

# Restrict inbound TCP/179 to your WireGuard network CIDR (recommended)
vault_bgp_router_manage_ufw: true
vault_bgp_router_allowed_source_cidr: "[vpn-network-cidr]"

vault_bgp_router_neighbors:
  - address: "[k8s-plane-wg-ip]"
    asn: "[metallb-my-asn]"
  - address: "[k8s-worker-wg-ip]"
    asn: "[metallb-my-asn]"

# =====================================================================
# BGP ROUTER HIGH AVAILABILITY (VRRP/Keepalived)
# =====================================================================
# For HA, deploy two FRR BGP routers with a floating VIP between them.
# MetalLB peers with BOTH router IPs (not the VIP).
# Upstream gateway routes to the VIP.
#
# Architecture:
#   Upstream -> BGP VIP -> [FRR Router 1 (MASTER) | FRR Router 2 (BACKUP)]
#                                    |                    |
#                               eBGP peering to MetalLB speakers
#
# Note: vault_bgp_keepalived_router_id must differ from vault_keepalived_router_id (K8s API VIP)

# Second BGP Router Configuration (optional, for HA)
# vault_bgp_router_2_asn: "[router-asn]"  # Same ASN as router 1
# vault_bgp_router_2_router_id: "[bgp-router-2-wg-ip]"
# vault_bgp_router_2_listen_interface: "[wg-interface]"
# vault_bgp_router_2_update_source: "[wg-interface]"
# vault_bgp_router_2_neighbors:
#   - address: "[k8s-plane-wg-ip]"
#     asn: "[metallb-my-asn]"
#   - address: "[k8s-worker-wg-ip]"
#     asn: "[metallb-my-asn]"

# BGP Router VIP (Keepalived VRRP) Configuration
vault_bgp_keepalived_vip: "[bgp-vip-address]"
vault_bgp_keepalived_vip_cidr: "32"
vault_bgp_keepalived_vip_interface: "[wg-interface]"
vault_bgp_keepalived_password: "[bgp-keepalived-password]"
vault_bgp_keepalived_router_id: "52" # Must differ from K8s API VIP router_id (51)
vault_bgp_keepalived_script_user: "[admin-username]"

# BGP Routers list for keepalived (required for HA)
# Each entry: name (inventory hostname), wireguard_ip, priority (higher = MASTER)
vault_bgp_routers:
  - name: "[bgp-router-1-hostname]"
    wireguard_ip: "[bgp-router-1-wg-ip]"
    priority: 150 # Higher priority = MASTER
  - name: "[bgp-router-2-hostname]"
    wireguard_ip: "[bgp-router-2-wg-ip]"
    priority: 100 # Lower priority = BACKUP

# Helm Configuration
vault_helm_enabled: true
vault_helm_operation: "install" # install, verify, remove
vault_helm_version: "latest" # or specific version like "v3.15.0"

# Traefik Ingress Controller (via Helm + MetalLB)
vault_traefik_enabled: true
vault_traefik_operation: "install" # install, verify, remove
vault_traefik_namespace: "traefik"
vault_traefik_release_name: "traefik"
vault_traefik_chart_ref: "traefik/traefik"
vault_traefik_chart_version: "" # empty = latest

# LoadBalancer behavior
# Leave empty for MetalLB auto-assign from your configured IPAddressPool.
vault_traefik_load_balancer_ip: ""

# Optional: force selection of a specific MetalLB pool
vault_traefik_metallb_address_pool_name: ""

# IngressClass behavior
vault_traefik_ingress_class_enabled: true
vault_traefik_ingress_class_is_default: true

# Security defaults
vault_traefik_dashboard_enabled: false

# HTTPS readiness (pair with cert-manager for real certificates)
vault_traefik_tls_enabled: true
vault_traefik_http_redirect_to_https: true

# ACME certificate resolver (Let's Encrypt) - alternative to cert-manager
vault_traefik_certresolver_enabled: false
vault_traefik_certresolver_name: "letsencrypt"
vault_traefik_acme_email: "[example@mail]"
vault_traefik_acme_storage: "/data/acme.json"
vault_traefik_acme_httpchallenge_entrypoint: "web"

# Removal behavior
vault_traefik_remove_namespace: false
vault_traefik_wait_for_external_ip: true

# cert-manager (via Helm) + Let's Encrypt staging ClusterIssuer
vault_cert_manager_enabled: true
vault_cert_manager_operation: "install" # install, verify, remove
vault_cert_manager_namespace: "cert-manager"
vault_cert_manager_release_name: "cert-manager"
vault_cert_manager_chart_ref: "jetstack/cert-manager"
vault_cert_manager_chart_version: "" # empty = latest
vault_cert_manager_helm_binary_path: "/usr/local/bin/helm"

# CRDs should be installed and kept (recommended)
vault_cert_manager_crds_enabled: true
vault_cert_manager_crds_keep: true

# ClusterIssuer: Let's Encrypt staging
vault_cert_manager_clusterissuer_staging_enabled: true
vault_cert_manager_clusterissuer_staging_name: "letsencrypt-staging"
vault_cert_manager_clusterissuer_staging_private_key_secret_name: "letsencrypt-staging"

# ACME account (use a real email in your vault)
vault_cert_manager_acme_email: "[your-email]"
vault_cert_manager_acme_server_staging: "https://acme-staging-v02.api.letsencrypt.org/directory"

# HTTP-01 solver ingress class (Traefik)
vault_cert_manager_http01_ingress_class: "traefik"

vault_cert_manager_wait_for_clusterissuer_ready: true
vault_cert_manager_remove_namespace: false

# PostgreSQL 17 in Docker
# Security defaults in postgres_docker_manage.yaml bind DB port to localhost.
# Set vault_postgres_bind_address to 0.0.0.0 only when remote clients must connect.
vault_postgres_container_name: "postgres17"
vault_postgres_image: "postgres:17"
vault_postgres_port: 5432
vault_postgres_bind_address: "127.0.0.1"
vault_postgres_db_name: "[postgres-database-name]"
vault_postgres_user: "[postgres-username]"
vault_postgres_password: "[postgres-strong-password-min-12-chars]"
vault_postgres_volume_name: "postgres17_data"
vault_postgres_network_name: "postgres17_net"
vault_postgres_env_file_path: "/etc/postgres_docker/postgres.env"
vault_postgres_pids_limit: 256
vault_postgres_memory: "1g"
# Firewall management (UFW)
# If UFW is installed and active, postgres_docker_manage.yaml will open vault_postgres_port/tcp.
# To restrict source access, set CIDRs in vault_postgres_allowed_networks.
# Leave list empty to allow from any source.
vault_postgres_manage_ufw: true
vault_postgres_allowed_networks: []

# Kubernetes verification settings for postgres_docker_verify.yaml
# Override target host only when the DB is not reachable via db group ansible_host.
vault_postgres_verify_namespace: "default"
# vault_postgres_verify_target_host: "[db-reachable-ip-or-dns]"

# Operation mode for postgres_docker_manage.yaml
# install: create/update container
# remove: remove container (volume/network/env file removal is optional and disabled by default)
vault_postgres_operation: "install"
vault_postgres_remove_env_file: false
vault_postgres_remove_volume: false
vault_postgres_remove_network: false
