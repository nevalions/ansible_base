---
# Example Ansible Vault secrets file for libssh connection
# This file should be encrypted with ansible-vault

# SSH key passphrase (for ~/.ssh/id_rsa)
vault_ssh_key_passphrase: "your_ssh_key_passphrase_here"

# Sudo/become password for workers_all
vault_become_pass: "your_sudo_password_here"

# Admin user for kubeconfig ownership
vault_admin_user: "[your-username]"

# Reserve SSH ports
vault_ssh_one: "[custom-ssh-port-1]"
vault_ssh_two: "[custom-ssh-port-2]"

# DNS Server Configuration
vault_dns_zone: "[dns-zone]"

vault_dns_servers:
  - "[primary-dns-hostname]"
  - "[secondary-dns-hostname]"

dns_upstrea_dns:
  - "[upstream-dns-1]"
  - "[upstream-dns-2]"

vault_dns_server_primary: "[primary-dns-server-ip]"
vault_dns_server_secondary: "[secondary-dns-server-ip]"

vault_dns_client_options:
  - "timeout:2"
  - "attempts:3"
  - "rotate"

vault_dns_records:
  # Node hostnames
  - type: "A"
    name: "[plane1-hostname]"
    ip: "[plane1-ip]"
  - type: "A"
    name: "[plane2-hostname]"
    ip: "[plane2-ip]"
  - type: "A"
    name: "[worker1-hostname]"
    ip: "[worker1-ip]"
  - type: "A"
    name: "[worker2-hostname]"
    ip: "[worker2-ip]"

  # Kubernetes services (external ingress/LoadBalancer)
  - type: "A"
    name: "[ingress-hostname]"
    ip: "[ingress-ip]"
  - type: "A"
    name: "[longhorn-hostname]"
    ip: "[longhorn-ip]"

  # Infrastructure services
  - type: "A"
    name: "[nfs-hostname]"
    ip: "[nfs-ip]"

# DNS configuration
vault_dns_allowed_networks:
  # Add all networks that should be able to query DNS
  # This list supports multiple networks for flexible deployment
  # - Always include localhost for local queries
  - "[physical-network-cidr]"
  - "[vpn-network-cidr]"
  - "[cloud-network-cidr]"
  - "[loopback-cidr]"

# Connection settings for DNS servers
vault_dns_ansible_user: "[username]"
vault_dns_ansible_port: "[ssh-port]"
vault_dns_ssh_key_path: "[/path/to/key]"

# WireGuard Configuration
vault_wg_interface: "[interface-name]"
vault_wg_server_port: "[server-port]"
vault_wg_client_default_port: "[client-default-port]"
vault_wg_client_port_start: "[port-range-start]"
vault_wg_client_port_end: "[port-range-end]"
vault_wg_network_cidr: "[vpn-network-cidr]"
vault_wg_server_ip: "[server-vpn-ip]"
vault_wg_dns_primary: "[dns-server-ip]"

# WireGuard Server Keys (auto-generated, store here after first deployment)
vault_wg_server_private_key: null
vault_wg_server_public_key: null

# WireGuard Peers Configuration
# Each peer maps to a host group in inventory
vault_wg_peers:
  - name: "[peer-name]"
    host_group: "[ansible-host-group]"
    allowed_ips: "[peer-vpn-ip]/32"
    endpoint: "[peer-public-ip]:[port]"
    client_listen_port: "[unique-port-for-nat-peers]"

# WireGuard Server IPs (multi-server mesh - maps inventory group to VPN IP)
vault_wg_server_ips:
  haproxy_hostname: "[haproxy-vpn-ip]"
  bgp_hostname: "[bgp-vpn-ip]"
  control_plane_hostname: "[control-plane-vpn-ip]"

# WireGuard Server Ports (multi-server mesh - maps inventory group to port)
vault_wg_server_ports:
  haproxy_hostname: "[haproxy-wg-port]"
  bgp_hostname: "[bgp-wg-port]"
  control_plane_hostname: "[plane-wg-port]"

# WireGuard Peer Keys (populated after key generation)
# Each node (server or client) has its own key pair
vault_wg_peer_private_keys:
  haproxy_hostname: "[haproxy-private-key]"
  bgp_hostname: "[bgp-private-key]"
  control_plane_hostname: "[plane-private-key]"
  worker_hostname: "[worker-private-key]"

vault_wg_peer_public_keys:
  haproxy_hostname: "[haproxy-public-key]"
  bgp_hostname: "[bgp-public-key]"
  control_plane_hostname: "[plane-public-key]"
  worker_hostname: "[worker-public-key]"

# WireGuard Firewall Configuration
vault_wg_allowed_networks:
  - "[physical-network-cidr-1]"
  - "[vpn-network-cidr]"
  - "[physical-network-cidr-2]"
  - "[physical-network-cidr-3]"
  - "[loopback-cidr]"

# HAProxy External Configuration (for ingress traffic only, NOT for k8s API HA)
# External HAProxy handles ports 80/443 for web traffic
vault_haproxy_k8s_frontend_port: "[haproxy-frontend-port]"
vault_haproxy_k8s_backend_port: "[haproxy-backend-port]"
vault_haproxy_k8s_backend_ip: "[control-plane-ip]"

# Keepalived VIP Configuration
# Simplified architecture: VIP floats between control planes via VRRP
# Workers connect to VIP -> traffic goes directly to kube-apiserver on MASTER node
# No HAProxy or DNAT required for internal k8s API HA
vault_keepalived_vip: "[vip-address]"
vault_keepalived_vip_cidr: "32"
vault_keepalived_vip_interface: "[vip-interface]"
vault_keepalived_password: "[keepalived-password]"
vault_keepalived_router_id: "51"
vault_keepalived_script_user: "[admin-username]"

# Kubernetes Version (use specific version to prevent accidental upgrades)
vault_kubernetes_version: "1.35"

# Kubernetes API VIP (for workers to join)
# This VIP is managed by Keepalived and floats between control planes
vault_k8s_api_vip: "[vip-address]"
vault_k8s_api_port: "[k8s-api-port]"

# Control Plane Configuration (multi-plane HA support)
# Add entries here for each control plane node
# - name: inventory hostname
# - wireguard_ip: node's WireGuard IP address
# - api_port: kube-apiserver port (use [k8s-api-port])
# - priority: VRRP priority (higher = preferred MASTER, e.g., 150, 140, 130)
#
# Example for 3-plane HA cluster:
# vault_k8s_control_planes:
#   - name: "[control-plane-hostname-1]"
#     wireguard_ip: "[control-plane-wg-ip-1]"
#     api_port: "[k8s-api-port]"
#     priority: 150
#   - name: "[control-plane-hostname-2]"
#     wireguard_ip: "[control-plane-wg-ip-2]"
#     api_port: "[k8s-api-port]"
#     priority: 140
#   - name: "[control-plane-hostname-3]"
#     wireguard_ip: "[control-plane-wg-ip-3]"
#     api_port: "[k8s-api-port]"
#     priority: 130
vault_k8s_control_planes:
  - name: "[control-plane-hostname]"
    wireguard_ip: "[control-plane-wg-ip]"
    api_port: "[k8s-api-port]"
    priority: 150

# Kubernetes Network Configuration
vault_k8s_pod_subnet: "[pod-network-cidr]"
vault_k8s_service_subnet: "[service-network-cidr]"

# Kubernetes Pre-flight Validation Configuration
vault_k8s_preflight_skip_docker_check: false
vault_k8s_preflight_skip_swap_check: false
vault_k8s_preflight_skip_port_check: false
vault_k8s_preflight_skip_cni_check: false
vault_k8s_preflight_skip_container_runtime_check: false
vault_k8s_preflight_skip_process_check: false
vault_k8s_preflight_fail_on_warnings: false

# Calico CNI Configuration
# Version of Calico to install (matches Tigera operator manifests)
vault_calico_version: "v3.31.3"

# IP Pool name
vault_calico_ippool_name: "default-ipv4-ippool"

# Encapsulation mode for pod traffic between nodes
# Options: "VXLANCrossSubnet", "VXLAN", "IPIPCrossSubnet", "IPIP", "None"
# - IPIP: Always use IPIP encapsulation (recommended for WireGuard networks)
# - IPIPCrossSubnet: IPIP only for cross-subnet traffic
# - VXLANCrossSubnet: VXLAN only for cross-subnet traffic
# - VXLAN: Always use VXLAN encapsulation
# - None: No encapsulation (requires L2 connectivity between nodes)
vault_calico_encapsulation: "IPIP"

# Enable NAT for pod traffic leaving the cluster
vault_calico_nat_outgoing: true

# Node selector for IP pool (which nodes can use this pool)
# "all()" means all nodes can use this IP pool
vault_calico_node_selector: "all()"

# Block size for per-node pod CIDR allocation (default /26 = 64 IPs per node)
vault_calico_block_size: 26

# MTU for Calico network interfaces (adjust for encapsulation overhead)
# - 1440 for VXLAN (1500 - 60 bytes overhead)
# - 1480 for IPIP (1500 - 20 bytes overhead)
# - Lower for WireGuard tunnels (e.g., 1300-1380)
vault_calico_mtu: 1440
