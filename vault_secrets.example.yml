---
# Example Ansible Vault secrets file for libssh connection
# This file should be encrypted with ansible-vault

# SSH key passphrase (for ~/.ssh/id_rsa)
vault_ssh_key_passphrase: "your_ssh_key_passphrase_here"

# Sudo/become password for workers_all
vault_become_pass: "your_sudo_password_here"

# Admin user for kubeconfig ownership
vault_admin_user: "[your-username]"

# Reserve SSH ports
vault_ssh_one: "[custom-ssh-port-1]"
vault_ssh_two: "[custom-ssh-port-2]"

# DNS Server Configuration
vault_dns_zone: "[dns-zone]"

vault_dns_servers:
  - "[primary-dns-hostname]"
  - "[secondary-dns-hostname]"

dns_upstrea_dns:
  - "[upstream-dns-1]"
  - "[upstream-dns-2]"

vault_dns_server_primary: "[primary-dns-server-ip]"
vault_dns_server_secondary: "[secondary-dns-server-ip]"

vault_dns_client_options:
  - "timeout:2"
  - "attempts:3"
  - "rotate"

vault_dns_records:
  # Kubernetes cluster DNS names (PRIMARY - use these for cluster)
  # API endpoint
  - type: "A"
    name: "k8s-api"
    ip: "[vip-address]"

  # Control plane nodes
  - type: "A"
    name: "k8s-plane1"
    ip: "[plane1-ip]"
  - type: "A"
    name: "k8s-plane2"
    ip: "[plane2-ip]"

  # Worker nodes
  - type: "A"
    name: "k8s-worker1"
    ip: "[worker1-ip]"
  - type: "A"
    name: "k8s-worker2"
    ip: "[worker2-ip]"

  # Legacy system hostnames (KEEP for compatibility/fallback)
  - type: "A"
    name: "[plane1-hostname]"
    ip: "[plane1-ip]"
  - type: "A"
    name: "[plane2-hostname]"
    ip: "[plane2-ip]"
  - type: "A"
    name: "[worker1-hostname]"
    ip: "[worker1-ip]"
  - type: "A"
    name: "[worker2-hostname]"
    ip: "[worker2-ip]"

  # Kubernetes services (external ingress/LoadBalancer)
  - type: "A"
    name: "[ingress-hostname]"
    ip: "[ingress-ip]"
  - type: "A"
    name: "[longhorn-hostname]"
    ip: "[longhorn-ip]"

  # Infrastructure services
  - type: "A"
    name: "[nfs-hostname]"
    ip: "[nfs-ip]"
  - type: "A"
    name: "vip"
    ip: "[vip-address]"

# DNS configuration
vault_dns_allowed_networks:
  # Add all networks that should be able to query DNS
  # This list supports multiple networks for flexible deployment
  # - Always include localhost for local queries
  - "[physical-network-cidr]"
  - "[vpn-network-cidr]"
  - "[cloud-network-cidr]"
  - "[loopback-cidr]"

# Connection settings for DNS servers
vault_dns_ansible_user: "[username]"
vault_dns_ansible_port: "[ssh-port]"
vault_dns_ssh_key_path: "[/path/to/key]"

# WireGuard Configuration
vault_wg_interface: "[interface-name]"
vault_wg_server_port: "[server-port]"
vault_wg_client_default_port: "[client-default-port]"
vault_wg_client_port_start: "[port-range-start]"
vault_wg_client_port_end: "[port-range-end]"
vault_wg_network_cidr: "[vpn-network-cidr]"
vault_wg_server_ip: "[server-vpn-ip]"
vault_wg_dns_primary: "[dns-server-ip]"

# WireGuard Server Keys (auto-generated, store here after first deployment)
vault_wg_server_private_key: null
vault_wg_server_public_key: null

# WireGuard Peers Configuration
# Each peer maps to a host group in inventory
#
# Required fields:
# - name: Unique identifier (must match vault_wg_peer_private_keys and vault_wg_peer_public_keys)
# - host_group: Maps to Ansible inventory group name
# - allowed_ips: VPN IP address for this peer (used in AllowedIPs)
# - endpoint: Public or internal IP:PORT for connecting
#
# Optional fields:
# - is_server: true for server nodes that route pod network traffic
#              When true, vault_wg_routed_cidrs (e.g., Calico pod CIDR) is added
#              to AllowedIPs in client configs. Set for control planes, HAProxy, BGP routers.
# - client_listen_port: UDP port for incoming WireGuard connections (for NAT peers)
# - dns_name: DNS name for this peer (for reference only, IPs used for endpoints)
#
# Example multi-server mesh with pod routing:
vault_wg_peers:
  # Servers (is_server: true) - route pod network traffic
  - name: "[haproxy-peer-name]"
    host_group: "[haproxy-host-group]"
    is_server: true
    allowed_ips: "[haproxy-vpn-ip]/32"
    endpoint: "[haproxy-public-ip]:[port]"
    dns_name: "haproxy.cluster.local"

  - name: "[bgp-peer-name]"
    host_group: "[bgp-host-group]"
    is_server: true
    allowed_ips: "[bgp-vpn-ip]/32"
    endpoint: "[bgp-internal-ip]:[port]"
    dns_name: "bgp.cluster.local"

  - name: "[control-plane-peer-name]"
    host_group: "[control-plane-host-group]"
    is_server: true
    allowed_ips: "[control-plane-vpn-ip]/32"
    endpoint: "[control-plane-internal-ip]:[port]"
    dns_name: "k8s-plane1.cluster.local"

  # Clients (no is_server) - workers, external nodes
  - name: "[worker-peer-name]"
    host_group: "[worker-host-group]"
    allowed_ips: "[worker-vpn-ip]/32"
    endpoint: "[worker-public-ip]:[port]"
    client_listen_port: "[unique-port-for-nat-peers]"
    dns_name: "k8s-worker1.cluster.local"

# WireGuard Server IPs (multi-server mesh - maps inventory group to VPN IP)
vault_wg_server_ips:
  haproxy_hostname: "[haproxy-vpn-ip]"
  bgp_hostname: "[bgp-vpn-ip]"
  control_plane_hostname: "[control-plane-vpn-ip]"

# WireGuard Server Ports (multi-server mesh - maps inventory group to port)
vault_wg_server_ports:
  haproxy_hostname: "[haproxy-wg-port]"
  bgp_hostname: "[bgp-wg-port]"
  control_plane_hostname: "[plane-wg-port]"

# WireGuard Peer Keys (populated after key generation)
# Each node (server or client) has its own key pair
vault_wg_peer_private_keys:
  haproxy_hostname: "[haproxy-private-key]"
  bgp_hostname: "[bgp-private-key]"
  control_plane_hostname: "[plane-private-key]"
  worker_hostname: "[worker-private-key]"

vault_wg_peer_public_keys:
  haproxy_hostname: "[haproxy-public-key]"
  bgp_hostname: "[bgp-public-key]"
  control_plane_hostname: "[plane-public-key]"
  worker_hostname: "[worker-public-key]"

# WireGuard Firewall Configuration
vault_wg_allowed_networks:
  - "[physical-network-cidr-1]"
  - "[vpn-network-cidr]"
  - "[physical-network-cidr-2]"
  - "[physical-network-cidr-3]"
  - "[loopback-cidr]"

# WireGuard Routed CIDRs
# Networks routed through WireGuard server peers (is_server: true)
# These CIDRs are automatically added to AllowedIPs for server peers
# Auto-populated by wireguard_manage.yaml from:
#   - vault_ipPools_cidr (Kubernetes pod network)
#   - vault_k8s_api_vip/32 (Kubernetes API VIP for HA)
# You typically don't need to set this manually
vault_wg_routed_cidrs:
  - "[pod-network-cidr]"  
  - "[k8s-api-vip]/32"    

# HAProxy External Configuration (for ingress traffic only, NOT for k8s API HA)
# External HAProxy handles ports 80/443 for web traffic
vault_haproxy_k8s_frontend_port: "[haproxy-frontend-port]"
vault_haproxy_k8s_backend_port: "[haproxy-backend-port]"
vault_haproxy_k8s_backend_ip: "[control-plane-ip]"

# Keepalived VIP Configuration
# Simplified architecture: VIP floats between control planes via VRRP
# Workers connect to VIP -> traffic goes directly to kube-apiserver on MASTER node
# No HAProxy or DNAT required for internal k8s API HA
vault_keepalived_vip: "[vip-address]"
vault_keepalived_vip_cidr: "32"
vault_keepalived_vip_interface: "[vip-interface]"
vault_keepalived_password: "[keepalived-password]"
vault_keepalived_router_id: "51"
vault_keepalived_script_user: "[admin-username]"

# Kubernetes Version (use specific version to prevent accidental upgrades)
vault_kubernetes_version: "1.35"

# Kubernetes API VIP (for workers to join)
# This VIP is managed by Keepalived and floats between control planes
vault_k8s_api_vip: "[vip-address]"
vault_k8s_api_port: "[k8s-api-port]"

# Kubernetes DNS configuration (NEW - for DNS-based cluster communication)
# Use these DNS names instead of IPs for cluster communication
# Set vault_k8s_use_dns_endpoint: true to enable DNS-based configuration
vault_k8s_dns_prefix: "k8s"
vault_k8s_api_dns: "k8s-api.{{ vault_dns_zone }}"
vault_k8s_api_dns_name: "k8s-api"
vault_k8s_use_dns_endpoint: true

# Control Plane Configuration (multi-plane HA support)
# Add entries here for each control plane node
# - name: inventory hostname
# - dns_name: DNS name for Kubernetes cluster (NEW)
# - wireguard_ip: node's WireGuard IP address
# - backend_port: Backend port (if using HAProxy)
# - api_port: kube-apiserver port (use [k8s-api-port])
# - priority: VRRP priority (higher = preferred MASTER, e.g., 150, 140, 130)
#
# Example for 3-plane HA cluster:
# vault_k8s_control_planes:
#   - name: "[control-plane-hostname-1]"
#     dns_name: "k8s-plane1"
#     wireguard_ip: "[control-plane-wg-ip-1]"
#     api_port: "[k8s-api-port]"
#     priority: 150
#   - name: "[control-plane-hostname-2]"
#     dns_name: "k8s-plane2"
#     wireguard_ip: "[control-plane-wg-ip-2]"
#     api_port: "[k8s-api-port]"
#     priority: 140
#   - name: "[control-plane-hostname-3]"
#     dns_name: "k8s-plane3"
#     wireguard_ip: "[control-plane-wg-ip-3]"
#     api_port: "[k8s-api-port]"
#     priority: 130
vault_k8s_control_planes:
  - name: "[control-plane-hostname]"
    dns_name: "k8s-plane1"
    wireguard_ip: "[control-plane-wg-ip]"
    backend_port: "[k8s-api-port]"
    api_port: "[k8s-api-port]"
    priority: 150

# Worker Configuration (NEW - for DNS-based worker configuration)
# Add entries here for each worker node
# - name: inventory hostname
# - dns_name: DNS name for Kubernetes cluster
# - wireguard_ip: node's WireGuard IP address
# - backend_port: Backend port (if using HAProxy)
# - api_port: Kubernetes API server port
#
# Example:
# vault_k8s_workers:
#   - name: "[worker-hostname-1]"
#     dns_name: "k8s-worker1"
#     wireguard_ip: "[worker-wg-ip-1]"
#     backend_port: "[k8s-api-port]"
#     api_port: "[k8s-api-port]"
#   - name: "[worker-hostname-2]"
#     dns_name: "k8s-worker2"
#     wireguard_ip: "[worker-wg-ip-2]"
#     backend_port: "[k8s-api-port]"
#     api_port: "[k8s-api-port]"
vault_k8s_workers:
  - name: "[worker-hostname]"
    dns_name: "k8s-worker1"
    wireguard_ip: "[worker-wg-ip]"
    backend_port: "[k8s-api-port]"
    api_port: "[k8s-api-port]"

# Kubernetes Network Configuration
vault_k8s_pod_subnet: "[pod-network-cidr]"
vault_k8s_service_subnet: "[service-network-cidr]"

# Kubernetes Pre-flight Validation Configuration
vault_k8s_preflight_skip_docker_check: false
vault_k8s_preflight_skip_swap_check: false
vault_k8s_preflight_skip_port_check: false
vault_k8s_preflight_skip_cni_check: false
vault_k8s_preflight_skip_container_runtime_check: false
vault_k8s_preflight_skip_process_check: false
vault_k8s_preflight_fail_on_warnings: false

# Calico CNI Configuration
# Version of Calico to install (matches Tigera operator manifests)
vault_calico_version: "v3.31.3"

# Calico node IP autodetection interface
vault_interface: "[interface-name]"

# IP Pool name
vault_ipPools: "default-ipv4-ippool"

# Pod CIDR for IP pool (should match vault_k8s_pod_subnet)
vault_ipPools_cidr: "[pod-network-cidr]"

# Encapsulation mode for pod traffic between nodes
# Options: "VXLANCrossSubnet", "VXLAN", "IPIPCrossSubnet", "IPIP", "None"
# - IPIP: Always use IPIP encapsulation (recommended for WireGuard networks)
# - IPIPCrossSubnet: IPIP only for cross-subnet traffic
# - VXLANCrossSubnet: VXLAN only for cross-subnet traffic
# - VXLAN: Always use VXLAN encapsulation
# - None: No encapsulation (requires L2 connectivity between nodes)
vault_ipPools_encapsulation: "IPIP"

# Enable NAT for pod traffic leaving the cluster
natOutgoing: true

# Node selector for IP pool (which nodes can use this pool)
# "all()" means all nodes can use this IP pool
nodeSelector: "all()"

# Block size for per-node pod CIDR allocation (default /26 = 64 IPs per node)
vault_ipPools_blockSize: 26

# MTU for Calico network interfaces (adjust for encapsulation overhead)
# - 1440 for VXLAN (1500 - 60 bytes overhead)
# - 1480 for IPIP (1500 - 20 bytes overhead)
# - Lower for WireGuard tunnels (e.g., 1300-1380)
mtu: 1440
